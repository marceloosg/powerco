---
title: "Power Co Exploratory Analysis"
output: html_notebook
---

#Libraries
```{r}
library(data.table)
library(RANN)
library(e1071)
source("functions.R")
library(doMC) 
registerDoMC(cores = 5) 
```



#Load Data
## Data Summary
```{r}
data=fread('train/ml_case_training_data.csv')
output=fread('train/ml_case_training_output.csv')
hist=fread('train/ml_case_training_hist_data.csv')
summary(data)
```
We can see a lot of missing values, the dataset must handle those case before any modeling can begin.

Let's take a look at the target variable
```{r}
table(output$churn)
```
We have umbalanced classes and we should undersample the data in order to prevent it from skewing the results.
But first, let's keep 30% of our data aside in order to have fresh data for the validation procedure.

# Validation Partitioning
```{r}
set.seed(814)
full_train_index=createDataPartition(output$churn,p=0.7,list = F)
full_train_data=data[full_train_index]
test_data=data[-full_train_index]
dim(full_train_data)
```
# Basic Cleaning
Let's us build a basic clean dataset before undersampling it.
This consists in converting dates, one_encoding categorical variables, and other minor feature transformations.
```{r}
clean_data=preprocess_train_pipeline(full_train_data,cleanOnly = T)
cdata=cbindlist(clean_data$data$output,append = T)
dim(cdata)
```
Now we have over 417 dimensions to deal with. The categorical variables have been transformed and each category have been assigned to a new binary variable.
To make column selection easier each variable had it's name preappended according to its origin
```{r}
unique(sapply(strsplit(names(cdata),split="\\."),function(s) s[[1]]))
```



# Undersampling partitions
```{r}
set.seed(815)
source("partition.R")
part=data_partition(full_train_data,output[full_train_index]$churn,k=11)
sampled_binary=part$apply(part,clean_data$data$combined$binaries,y=T)
lapply(sampled_binary,dim)
```

We chose to undersample before making any descriptive analysis to prevent long tailed distribuition from skewing results due to the large samples of a specific class.

Let's see which are the top 10 binary values with great jaccard similarity with our target variable:
# Similarities
```{r}
bin=part$measure(sampled_binary,jaccard)
bin
```
These results were averaged out of 11 resamplings.

#Accuracy
```{r}
part$measure(sampled_binary,accuracy)
```

Now let's look at time dependence of the average of our target variable:
```{r}
sampled_values=part$apply(part,clean_data$data$raw$values,y=T)
plots=data_partition.regression(sampled_values)
plot_grid(plotlist=plots)
```
From the last couple of years we can see a linear increase in the probability of churn with time. The means that recent activation users are more likely to churn.

```{r}
hist.sampled=data_partition.histogram(sampled_values)
ggplot(hist.sampled,aes(x=x,y=N,fill=y,ymax=Nmax,ymin=Nmin))+
  geom_histogram(stat='identity',position='identity',alpha=0.5)+
  geom_errorbar()+geom_vline(xintercept=200900)+geom_vline(xintercept=201133)
```
```{r}
hist_sample=part$apply(part,cbind(clean_data$data$raw$values,
                     clean_data$data$combined$binaries[,.(origin=origin_up.lxidpiddsbxsbosboudacockeimpuepw)]),y=T)
hist.sampled_origin=data_partition.histogram_origin(hist_sample)
ggplot(hist.sampled_origin,aes(x=x,y=N,fill=y,ymax=Nmax,ymin=Nmin))+
  geom_histogram(stat='identity',position='identity',alpha=0.5)+
  geom_errorbar()+geom_vline(xintercept=200900)+geom_vline(xintercept=201133)+facet_wrap(~origin)
```
```{r}
hist_sample=part$apply(part,cbind(clean_data$data$raw$values,
                     clean_data$data$combined$binaries[,.(has_gas,
                                                          origin=origin_up.lxidpiddsbxsbosboudacockeimpuepw)]),y=T)
hist.sampled_origin=data_partition.histogram_origin_has_gas(hist_sample)
ggplot(hist.sampled_origin[has_gas==1],aes(x=x,y=N,fill=y,ymax=Nmax,ymin=Nmin))+
  geom_histogram(stat='identity',position='identity',alpha=0.5)+
  geom_errorbar()+geom_vline(xintercept=200900)+geom_vline(xintercept=201133)+facet_wrap(has_gas~origin)
```
```{r}
set.seed(816)
imput_data=preprocess_train_pipeline(full_train_data)
idata=predict_preprocess_model(imput_data,full_train_data)

hist_sample=part$apply(part,cbind(imput_data$data$output$imputed_values,
                     clean_data$data$combined$binaries[,.(has_gas,
                                                          origin=origin_up.lxidpiddsbxsbosboudacockeimpuepw)]),y=T)
ggplot(hist_sample$Resample.1[,.N,.(y,cut(forecast_base_bill_year,30))][order(cut)],
       aes(x=cut,y=N,fill=y))+geom_histogram(stat='identity',position="dodge")
```


Let's see if the energy price follows the same trend:

```{r}
dt=cbind(clean_data$data$output$id,clean_data$data$raw$values)
dt=merge(dt,output,by='id')
clean_hist=hist
clean_hist[,price_date:= as.Date(price_date,format="%Y-%m-%d")]
clean_hist=melt(clean_hist,id.vars=c('id','price_date'))
clean_hist[,sd:=sd(value),.(variable,id)]

forecast=clean_hist[sd > 0][,.(price_2017=predict(lm(value ~ price_date),newdata = data.table(price_date=as.Date("2017-01-01")))),.(id,variable)]
still_price_forecast=clean_hist[sd == 0][, .(price_2017=mean(value,na.rm=T)),.(id,variable)]
full_forecast=rbind(forecast,still_price_forecast)
avg_price_forecast=clean_hist[, .(avg_price_2015=mean(value,na.rm=T)),.(id,variable)]
full_forecast=merge(full_forecast,avg_price_forecast,by=c('id','variable'))
forecast_values=merge(full_forecast,dt,by='id',all=F)

origin=cbind(clean_data$data$output$id,clean_data$data$combined$binaries[,.(origin=origin_up.lxidpiddsbxsbosboudacockeimpuepw)])
forecast_values=merge(forecast_values,origin,by='id',all=F)

dt_plot=forecast_values[,.(.N,price_2017=mean(price_2017),sd=sd(price_2017)) ,.(x=activ_year*100+(activ_month/12-0.5/12)*100,variable,churn)]
ggplot(dt_plot,aes(x=x,y=price_2017,ymin=price_2017-sd,ymax=price_2017+sd,color=variable,weight=N))+geom_point()+geom_errorbar()+geom_smooth()+facet_grid(~churn)+ylim(0,60)
```
```{r}
dt_plot=forecast_values[,.(.N,price_2017=mean(price_2017),sd=sd(price_2017)) ,.(x=activ_year*100+(activ_month/12-0.5/12)*100,variable,churn,origin)]
ggplot(dt_plot,aes(x=x,y=price_2017,ymin=price_2017-sd,ymax=price_2017+sd,color=variable,weight=N))+geom_point()+geom_errorbar()+geom_smooth()+facet_grid(origin~churn)+ylim(0,60)
```


```{r}
forepart=part$apply(part,forecast_values,y=T,by.id=T)
forefunc=function(foreplot,name){
foreplot=foreplot[activ_year > 2008,.(N=length(unique(id))),.(x=activ_year*100+floor(activ_month/4-0.25)*4/12*100,
variable,                                                        price_2017=cut(price_2017,c(-46,14,28,42,112)),churn=as.factor(churn))]
foreplot$name=name
foreplot
}
foremerge=rbindlist(lapply(names(forepart),function(name) forefunc(forepart[[name]],name)))
foremerge=foremerge[,.(Nsd=sd(N),N=mean(N)),.(x,variable,price_2017,churn)]
ggplot(foremerge,aes(x=x,y=N,fill=churn,ymin=N-Nsd,ymax=N+Nsd))+
  geom_histogram(stat='identity',position='identity',alpha=0.5)+geom_errorbar()+
  facet_grid(variable ~price_2017)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
```{r}
forepart=part$apply(part,forecast_values,y=T,by.id=T)
forefunc=function(foreplot,name){
foreplot=foreplot[activ_year > 2008,.(N=length(unique(id))),.(x=activ_year > 2011 | activ_year == 2011 & activ_month > 3,
price_2017=cut(price_2017,c(-46,-0.1,-0.05,0.05,0.1,16:17,24,35,40:45,112)),churn=as.factor(churn),origin)]
foreplot$name=name
foreplot
}
foremerge=rbindlist(lapply(names(forepart),function(name) forefunc(forepart[[name]],name)))
foremerge=foremerge[,.(Nsd=sd(N),N=mean(N)),.(x,price_2017,churn,origin)]
ggplot(foremerge,aes(x=price_2017,y=N,fill=churn,ymin=N-Nsd,ymax=N+Nsd))+
  geom_histogram(stat='identity',position='identity',alpha=0.5)+geom_errorbar()+
  facet_grid(origin~x )+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
forepart=part$apply(part,forecast_values,y=T,by.id=T)
forefunc=function(foreplot,name){
foreplot=foreplot[activ_year > 2008,
                  .(N=length(unique(id))),
                  .(x=activ_year > 2011 | activ_year == 2011 & activ_month > 3,
                    imp_cost=cut(log(imp_cons-min(imp_cons)+1),30),
                    churn=as.factor(churn),
                    origin)]
foreplot$name=name
foreplot
}
foremerge=rbindlist(lapply(names(forepart),function(name) forefunc(forepart[[name]],name)))
foremerge=foremerge[,.(Nsd=sd(N),N=mean(N)),.(x,origin,imp_cost,churn)]
ggplot(foremerge,aes(x=imp_cost,y=N,fill=churn,ymin=N-Nsd,ymax=N+Nsd))+
  geom_histogram(stat='identity',position='identity',alpha=0.5)+geom_errorbar()+
  facet_grid(origin~x )+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```




Not much help, we should look for the price distribuition if it changes over time.

```{r}
dt_cat_plot=forecast_values[,.(price_cat=(as.character(cut(price_2017,4))),activ_year,activ_month,id,churn),variable]
dt_cat_plot=forecast_values[,.(price_cat=cut(price_2017,breaks=c(-50,0,0.05,0.1,1,10,20,30,40,50,60,70,80,105)),activ_year,activ_month,id,churn,variable)]

dt_cat_plot=dt_cat_plot[,.N ,.(x=activ_year,variable,price_cat,churn)]
dt_cat_plot=dt_cat_plot[,.(p=N/sum(N),price_cat) ,.(x,variable,churn)]

ggplot(dt_cat_plot,aes(x=x,y=p,fill=price_cat))+geom_bar(stat='identity')+facet_grid(churn~variable)+coord_flip()+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
Does gas_consumtion is a confounding variable?
```{r}
hist(log(forecast_values$cons_gas_12m+1))
```

```{r}
gas_values=merge(forecast_values,data[,.(id,has_gas)],by='id')
dt_plot=gas_values[,.(.N,avg_churn=mean(churn)),.(price_cat=cut(price_2017,breaks = c(-46,0,4,8,12,24,48,105)),gas_cat=cut(transform_gas(cons_gas_12m),c(-8,-4,-2,0,1,2,6,15)),has_gas)]
dt_plot=dt_plot[,.(N,p=N/sum(N),avg_churn,gas_cat,price_cat),has_gas]
ggplot(dt_plot,aes(x=price_cat,y=avg_churn,fill=gas_cat))+geom_bar(stat='identity',position='dodge')+facet_wrap(~has_gas)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+geom_point(aes(y=p,color=gas_cat,size=N))
```

#Let's see the difference between the churn distribuition across prices, time and if it is a gas client.
```{r}
dt_cat_plot=gas_values[,.(price_cat=cut(price_2017,breaks=c(-50,0,0.05,0.1,1,10,20,30,40,50,60,70,80,105)),has_gas,activ_year,activ_month,id,churn,variable)]

dt_cat_plot=dt_cat_plot[,.N ,.(x=activ_year,variable,price_cat,churn,has_gas)]
dt_cat_plot=dt_cat_plot[,.(N,p=N/sum(N),price_cat) ,.(x,variable,churn,has_gas)]

delta_plot=merge(dt_cat_plot[churn==0 ,-c("churn")],dt_cat_plot[churn==1][,-c("churn")],by=c("x","variable","price_cat","has_gas"),all=T)
delta_plot[!complete.cases(delta_plot)]
delta_plot[is.na(p.x)]$p.x=0
delta_plot[is.na(p.y)]$p.y=0
delta_plot[,p:=p.y-p.x]

ggplot(delta_plot,aes(x=x,y=p,fill=price_cat))+geom_bar(stat='identity')+facet_grid(has_gas~variable)+coord_flip()+theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```
These cannot explain the increase in churn in the recent years.
```{r}
adt_plot=gas_values[,.(.N,price_2017=mean(price_2017),sd=sd(price_2017)) ,.(x=activ_year+0*(activ_month/12-0.5/12)*100,variable,churn,has_gas,
                                                                         gas_cat=cut(transform_gas(cons_gas_12m),c(-8,0,1,2,4,12,15)))]
dt_plot=merge(adt_plot[churn==0],adt_plot[churn==1],all=T,by=c('x','variable','has_gas','gas_cat'))
dt_plot[is.na(price_2017.x)]$price_2017.x =0 
dt_plot[is.na(price_2017.y)]$price_2017.y =0 
dt_plot[is.na(sd.x)]$sd.x =0 
dt_plot[is.na(sd.y)]$sd.y =0 

dt_plot[,price_2017:=price_2017.y- price_2017.x]
dt_plot[,sd:=sd.y + sd.x]
dt_plot[,has_churn:=!is.na(churn.y)]
ggplot(dt_plot,aes(x=x,y=price_2017,ymin=price_2017-sd,ymax=price_2017+sd,color=variable,size=N.x),shape=4)+geom_point(shape=4)+geom_errorbar(size=1,alpha=0.5)+
  facet_grid(gas_cat~has_gas+has_churn)+theme(axis.text.x = element_text(angle = 90, hjust = 1))+geom_hline(yintercept=0)+ylim(-20,20)

```
```{r}
dt_plot[,sum((price_2017 > 0 )*N.x,na.rm=T)/sum(N.x,na.rm=T)]
```
80% of churns have average price above the normal average.

TODO : calculate price increase
```{r}
gas_values=merge(forecast_values,data[,.(id,has_gas)],by='id')
dt_plot=gas_values[activ_year >= 2009,.(.N,avg_churn=mean(churn)),.(x=activ_year,price_cat=cut(price_2017,breaks = c(-46,0,4,8,12,24,48,105)),gas_cat=cut(transform_gas(cons_gas_12m),c(-8,-4,-2,0,1,2,15)),has_gas)]
dt_plot=dt_plot[,.(N,p=N/sum(N),price_cat,gas_cat,avg_churn),.(x,has_gas)]
ggplot(dt_plot,aes(x=price_cat,y=avg_churn,fill=gas_cat))+geom_bar(stat='identity',position='dodge')+geom_point(aes(y=p,color=gas_cat))+
  facet_wrap(x~has_gas)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
```{r}
gas_values=merge(forecast_values,data[,.(id,has_gas)],by='id')
dt_plot=gas_values[activ_year >= 2009,.(.N,avg_churn=mean(churn)),.(x=activ_year,price_cat=cut(price_2017,breaks = c(-46,0,4,8,12,24,48,105)),gas_cat=cut(transform_gas(cons_gas_12m),c(-8,-4,-2,0,1,2,15)),has_gas)]
dt_plot=dt_plot[,.(N,p=N/sum(N),price_cat,gas_cat,avg_churn),.(x,has_gas)]
ggplot(dt_plot,aes(x=as.numeric(price_cat),y=avg_churn,color=gas_cat,size=p))+geom_point()+
  facet_wrap(x~has_gas)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
gas_values=merge(forecast_values,data[,.(id,has_gas)],by='id')
ggplot(gas_values[,.(.N,avg_churn=mean(churn)),.(x=activ_year,price_cat=cut(price_2017,breaks = c(-46,0,4,8,12,24,48,105)),gas_cat=cut(log(cons_gas_12m+1),c(-1,0,1,2,4,6,8,10,12,14)),has_gas)],aes(x=price_cat,y=N,fill=gas_cat))+geom_bar(stat='identity',position='dodge')+facet_wrap(x~has_gas)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}

gas_values[,summary(transform_gas(cons_gas_12m))]
dt_plot=gas_values[activ_year >= 2009,.N,.(x=activ_year,price_cat=cut(price_2017,breaks = c(-46,0,4,8,12,24,48,105)),gas_cat=cut(transform_gas(cons_gas_12m),c(-8,-4,-2,0,1,2,15)),has_gas,churn)]
dt_plot=dt_plot[,.(N,p=N/sum(N),price_cat,gas_cat),.(x,has_gas,churn)]
ggplot(dt_plot,aes(x=price_cat,y=p,fill=gas_cat))+geom_bar(stat='identity',position='dodge')+
facet_grid(x~has_gas+churn)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))

#+geom_line(aes(y=p,x=as.numeric(price_cat),color=gas_cat))+
  
```


```{r,cache=TRUE}

preprocess_model=preprocess_train_pipeline(data)
```

```{r}
save(preprocess_model,file="preprocessing.model.RData")
```


```{r,cache=TRUE}
load("preprocessing.model.RData")
plot_explain_variance(preprocess_model)
```

```{r}
plot_cum_explain_variance(preprocess_model)
```

# Let's see how accurate pca can reconstruct the binary matrix with just 15 dimensions
```{r}
ret=pca_accuracy(preprocess_model)
preprocess_model=ret[[1]]
plot_grid(plotlist=list(ret[[2]],ret[[3]]))
```

# Are the low accuracy columns those correlated?
```{r}
corrplot::corrplot(cor(preprocess_model$data$combined$binaries[,preprocess_model$fit$pca$bin_accuracy[accurate==F]$names,with=F]))
```

```{r}
ret=pca_accuracy(preprocess_model,F)
preprocess_model=ret[[1]]
plot_grid(plotlist=list(ret[[2]],ret[[3]]))
```

# Are the low accuracy columns those correlated?
```{r}
corrplot::corrplot(cor(preprocess_model$data$output$imputed_values[,preprocess_model$fit$pca$val_accuracy[accurate==F]$names,with=F]))
```
