---
title: "Power Co Exploratory Analysis"
output: html_notebook
---

#Libraries
```{r}
library(data.table)
library(RANN)
library(e1071)
source("functions.R")
```



#Load Data
## Data Summary
```{r}
data=fread('train/ml_case_training_data.csv')
output=fread('train/ml_case_training_output.csv')
hist=fread('train/ml_case_training_hist_data.csv')
summary(data)
```
We can see a lot of missing values, the dataset must handle those case before any modeling can begin.

Let's take a look at the target variable
```{r}
table(output$churn)
```
We have umbalanced classes and we should undersample the data in order to prevent it from skewing the results.
But first, let's keep 30% of our data aside in order to have fresh data for the validation procedure.

# Validation Partitioning
```{r}
set.seed(814)
full_train_index=createDataPartition(output$churn,p=0.7,list = F)
full_train_data=data[full_train_index]
test_data=data[-full_train_index]
```
# Basic Cleaning
Let's us build a basic clean dataset before undersampling it.
This consists in converting dates, one_encoding categorical variables, and other minor feature transformations.
```{r}
clean_data=preprocess_train_pipeline(full_train_data,cleanOnly = T)
cdata=cbindlist(clean_data$data$output,append = T)
dim(cdata)
```
Now we have over 417 dimensions to deal with. The categorical variables have been transformed and each category have been assigned to a new binary variable.
To make column selection easier each variable had it's name preappended according to its origin
```{r}
unique(sapply(strsplit(names(cdata),split="\\."),function(s) s[[1]]))
```



# Undersampling partitions
```{r}
set.seed(815)
source("partition.R")
part=data_partition(full_train_data,output[full_train_index]$churn,k=11)
sampled_binary=part$apply(part,clean_data$data$combined$binaries,y=T)
lapply(sampled_binary,dim)
```

We chose to undersample before making any descriptive analysis to prevent long tailed distribuition from skewing results due to the large samples of a specific class.

Let's see which are the top 10 binary values with great jaccard similarity with our target variable:
# Similarities
```{r}
part$measure(sampled_binary,jaccard)
```
These results were averaged out of 11 resamplings.

#Accuracy
```{r}
part$measure(sampled_binary,accuracy)
```

Now let's look at time dependence of the average of our target variable:
```{r}
sampled_values=part$apply(part,clean_data$data$raw$values,y=T)
plots=data_partition.regression(sampled_values)
plot_grid(plotlist=plots)
```
From the last couple of years we can see a linear increase in the probability of churn with time. The means that recent activation users are more likely to churn.

Let's see if the energy price follows the same trend:

```{r}
dt=cbind(clean_data$data$output$id,clean_data$data$raw$values)
dt=merge(dt,output,by='id')
clean_hist=hist
clean_hist[,price_date:= as.Date(price_date,format="%Y-%m-%d")]
clean_hist=melt(clean_hist,id.vars=c('id','price_date'))
clean_hist[,sd:=sd(value),.(variable,id)]
forecast=clean_hist[sd > 0][,.(price_2017=predict(lm(value ~ price_date),newdata = data.table(price_date=as.Date("2017-01-01")))),.(id,variable)]
still_price_forecast=clean_hist[sd == 0][, .(price_2017=mean(value,na.rm=T)),.(id,variable)]
full_forecast=rbind(forecast,still_price_forecast)
avg_price_forecast=clean_hist[, .(avg_price_2015=mean(value,na.rm=T)),.(id,variable)]
full_forecast=merge(full_forecast,avg_price_forecast,by=c('id','variable'))
forecast_values=merge(full_forecast,dt,by='id',all=F)
dt_plot=forecast_values[,.(price_2017=mean(price_2017),sd=sd(price_2017)) ,.(x=activ_year*100+(activ_month/12-0.5/12)*100,variable)]
ggplot(dt_plot,aes(x=x,y=price_2017,ymin=price_2017-sd,ymax=price_2017+sd,color=variable))+geom_point()+geom_errorbar()
```
```{r}
dt_plot=forecast_values[,.(avg_price_2015=mean(avg_price_2015),sd=sd(avg_price_2015))
                        ,.(x=activ_year*100+(activ_month/12-0.5/12)*100,variable)]

ggplot(dt_plot,aes(x=x,y=avg_price_2015,ymin=avg_price_2015-sd,ymax=avg_price_2015+sd))+geom_point()+geom_errorbar()
```



Not much help, we should look for the price distribuition if it changes over time.

```{r}
dt_cat_plot=forecast_values[,.(price_cat=(as.character(cut(price_2017,4))),activ_year,activ_month,id,churn),variable]
dt_cat_plot=forecast_values[,.(price_cat=cut(price_2017,breaks=c(-50,0,0.05,0.1,1,10,20,30,40,50,60,70,80,105)),activ_year,activ_month,id,churn,variable)]

dt_cat_plot=dt_cat_plot[,.N ,.(x=activ_year,variable,price_cat,churn)]
dt_cat_plot=dt_cat_plot[,.(p=N/sum(N),price_cat) ,.(x,variable,churn)]

ggplot(dt_cat_plot,aes(x=x,y=p,fill=price_cat))+geom_bar(stat='identity')+facet_grid(churn~variable)+coord_flip()+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
Does gas_consumtion is a confounding variable?
```{r}
hist(log(forecast_values$cons_gas_12m+1))
```

```{r}
gas_values=merge(forecast_values,data[,.(id,has_gas)],by='id')
dt_plot=gas_values[,.(.N,avg_churn=mean(churn)),.(price_cat=cut(price_2017,breaks = c(-46,0,4,8,12,24,48,105)),gas_cat=cut(transform_gas(cons_gas_12m),c(-8,-4,-2,0,1,2,6,15)),has_gas)]
dt_plot=dt_plot[,.(N,p=N/sum(N),avg_churn,gas_cat,price_cat),has_gas]
ggplot(dt_plot,aes(x=price_cat,y=avg_churn,fill=gas_cat))+geom_bar(stat='identity',position='dodge')+facet_wrap(~has_gas)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+geom_point(aes(y=p,color=gas_cat,size=N))
```

#Let's see the difference between the churn distribuition across prices, time and if it is a gas client.
```{r}
dt_cat_plot=gas_values[,.(price_cat=cut(price_2017,breaks=c(-50,0,0.05,0.1,1,10,20,30,40,50,60,70,80,105)),has_gas,activ_year,activ_month,id,churn,variable)]

dt_cat_plot=dt_cat_plot[,.N ,.(x=activ_year,variable,price_cat,churn,has_gas)]
dt_cat_plot=dt_cat_plot[,.(N,p=N/sum(N),price_cat) ,.(x,variable,churn,has_gas)]

delta_plot=merge(dt_cat_plot[churn==0 ,-c("churn")],dt_cat_plot[churn==1][,-c("churn")],by=c("x","variable","price_cat","has_gas"),all=T)
delta_plot[!complete.cases(delta_plot)]
delta_plot[is.na(p.x)]$p.x=0
delta_plot[is.na(p.y)]$p.y=0
delta_plot[,p:=p.y-p.x]

ggplot(delta_plot,aes(x=x,y=p,fill=price_cat))+geom_bar(stat='identity')+facet_grid(has_gas~variable)+coord_flip()+theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```
These cannot explain the increase in churn in the recent years.
```{r}
adt_plot=gas_values[,.(.N,price_2017=mean(price_2017),sd=sd(price_2017)) ,.(x=activ_year+0*(activ_month/12-0.5/12)*100,variable,churn,has_gas,
                                                                         gas_cat=cut(transform_gas(cons_gas_12m),c(-8,0,1,2,4,12,15)))]
dt_plot=merge(adt_plot[churn==0],adt_plot[churn==1],all=T,by=c('x','variable','has_gas','gas_cat'))
dt_plot[is.na(price_2017.x)]$price_2017.x =0 
dt_plot[is.na(price_2017.y)]$price_2017.y =0 
dt_plot[is.na(sd.x)]$sd.x =0 
dt_plot[is.na(sd.y)]$sd.y =0 

dt_plot[,price_2017:=price_2017.y- price_2017.x]
dt_plot[,sd:=sd.y + sd.x]
dt_plot[,has_churn:=!is.na(churn.y)]
ggplot(dt_plot,aes(x=x,y=price_2017,ymin=price_2017-sd,ymax=price_2017+sd,color=variable,size=N.x),shape=4)+geom_point(shape=4)+geom_errorbar(size=1,alpha=0.5)+
  facet_grid(gas_cat~has_gas+has_churn)+theme(axis.text.x = element_text(angle = 90, hjust = 1))+geom_hline(yintercept=0)+ylim(-20,20)

```
```{r}
dt_plot[,sum((price_2017 > 0 )*N.x,na.rm=T)/sum(N.x,na.rm=T)]
```
80% of churns have average price above the normal average.

```{r}
gas_values=merge(forecast_values,data[,.(id,has_gas)],by='id')
dt_plot=gas_values[activ_year >= 2009,.(.N,avg_churn=mean(churn)),.(x=activ_year,price_cat=cut(price_2017,breaks = c(-46,0,4,8,12,24,48,105)),gas_cat=cut(transform_gas(cons_gas_12m),c(-8,-4,-2,0,1,2,15)),has_gas)]
dt_plot=dt_plot[,.(N,p=N/sum(N),price_cat,gas_cat,avg_churn),.(x,has_gas)]
ggplot(dt_plot,aes(x=price_cat,y=avg_churn,fill=gas_cat))+geom_bar(stat='identity',position='dodge')+geom_point(aes(y=p,color=gas_cat))+
  facet_wrap(x~has_gas)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
```{r}
gas_values=merge(forecast_values,data[,.(id,has_gas)],by='id')
dt_plot=gas_values[activ_year >= 2009,.(.N,avg_churn=mean(churn)),.(x=activ_year,price_cat=cut(price_2017,breaks = c(-46,0,4,8,12,24,48,105)),gas_cat=cut(transform_gas(cons_gas_12m),c(-8,-4,-2,0,1,2,15)),has_gas)]
dt_plot=dt_plot[,.(N,p=N/sum(N),price_cat,gas_cat,avg_churn),.(x,has_gas)]
ggplot(dt_plot,aes(x=as.numeric(price_cat),y=avg_churn,color=gas_cat,size=p))+geom_point()+
  facet_wrap(x~has_gas)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
gas_values=merge(forecast_values,data[,.(id,has_gas)],by='id')
ggplot(gas_values[,.(.N,avg_churn=mean(churn)),.(x=activ_year,price_cat=cut(price_2017,breaks = c(-46,0,4,8,12,24,48,105)),gas_cat=cut(log(cons_gas_12m+1),c(-1,0,1,2,4,6,8,10,12,14)),has_gas)],aes(x=price_cat,y=N,fill=gas_cat))+geom_bar(stat='identity',position='dodge')+facet_wrap(x~has_gas)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
transform_gas=function(gas){
  sapply(gas,function(s){
    ifelse(s < 0, log(-1/s),
           ifelse(s == 0, 0, log(s)))
  })
}
gas_values[,summary(transform_gas(cons_gas_12m))]
dt_plot=gas_values[activ_year >= 2009,.N,.(x=activ_year,price_cat=cut(price_2017,breaks = c(-46,0,4,8,12,24,48,105)),gas_cat=cut(transform_gas(cons_gas_12m),c(-8,-4,-2,0,1,2,15)),has_gas,churn)]
dt_plot=dt_plot[,.(N,p=N/sum(N),price_cat,gas_cat),.(x,has_gas,churn)]
ggplot(dt_plot,aes(x=price_cat,y=p,fill=gas_cat))+geom_bar(stat='identity',position='dodge')+
facet_grid(x~has_gas+churn)+ theme(axis.text.x = element_text(angle = 90, hjust = 1))

#+geom_line(aes(y=p,x=as.numeric(price_cat),color=gas_cat))+
  
```


```{r,cache=TRUE}

preprocess_model=preprocess_train_pipeline(data)
```

```{r}
save(preprocess_model,file="preprocessing.model.RData")
```


```{r,cache=TRUE}
load("preprocessing.model.RData")
plot_explain_variance(preprocess_model)
```

```{r}
plot_cum_explain_variance(preprocess_model)
```

# Let's see how accurate pca can reconstruct the binary matrix with just 15 dimensions
```{r}
pca_accuracy(preprocess_model)
```

# Are the low accuracy columns those correlated?
```{r}
corrplot::corrplot(cor(preprocess_model$data$combined$binaries[,preprocess_model$fit$pca$bin_accuracy[accurate==F]$names,with=F]))
```

```{r}
pca_accuracy(preprocess_model,F)
```

# Are the low accuracy columns those correlated?
```{r}
corrplot::corrplot(cor(preprocess_model$data$output$imputed_values[,preprocess_model$fit$pca$val_accuracy[accurate==F]$names,with=F]))
```

```{r}

preprocessed_training_data=preprocess_predict_pipeline(preprocess_model,data)
dim(preprocessed_training_data$results)
```

#Exploration:
```{r}
data.table(t(preprocessed_training_data$data$raw$values[,lapply(.SD,function(s) cor(s[!is.na(s)],output[!is.na(s)]$churn))]),keep.rownames = T)[order(-abs(V1))]
```
```{r}
values=(data.table(t(preprocessed_training_data$data$raw$values[,lapply(.SD,function(s) 
  c("m"=mean(s,na.rm=T),"s"=sd(s,na.rm=T))
  ), output$churn]),keep.rownames = T))
v=values[rn!="output",.(rn,r=V3/V4-V1/V2)][order(-abs(r))]
v
```
```{r}
library(MASS)
dd=cbind(preprocessed_training_data$data$raw$values[,v[1:3]$rn,with=F],output[,churn])
dd=dd[complete.cases(dd)]
ds=rbind(dd[V2==1],dd[V2==0][sample(1:dim(dd[V2==0])[1],size=dim(dd[V2==1])[1])])
lmod=randomForest::randomForest(factor(V2) ~ .,data=ds)




```


```{r}
date_churn=cbind(preprocess_model$data$output$id,get_dates(data,preprocess_model$fit$structure_model$indexes,F))
date_churn=merge(date_churn,output,by="id")
date_churn[,mean(churn),month(date_end)][order(-V1)]
```
```{r}

```


# Training
## split
```{r}
result=merge(preprocessed_training_data$results,output,by="id")
set.seed(814)
index = createDataPartition(y=output$churn, p=0.7, list=FALSE )

train = result[index,-c("id")]
test = result[-index,-c("id")]
```

#Baysian
```{r}
set.seed(814)
nb.fit = train(as.factor(churn) ~ ., data=train, method="nb",
                trControl = trainControl(method = "cv"))
z=predict(nb.fit)
confusionMatrix(z,as.factor(train$churn))
```

#LDA
```{r}
set.seed(814)
lda.fit = train(as.factor(churn) ~ ., data=train, method="lda",
                trControl = trainControl(method = "cv"))
z=predict(lda.fit)
confusionMatrix(z,factor(train$churn))
```

```{r}
set.seed(814)
ctrl <- trainControl(method = "cv", 
                     number = 7, 
                     verboseIter = FALSE,
                     sampling = "down")
down.lda.fit = train(as.factor(churn) ~ ., data=train, method="lda",
                trControl = ctrl)
z=predict(down.lda.fit)
confusionMatrix(z,factor(train$churn))
```
```{r}
set.seed(814)
ctrl <- trainControl(method = "cv", 
                     number = 7, 
                     verboseIter = FALSE,
                     sampling = "up")
lda.fit = train(as.factor(churn) ~ ., data=train, method="lda",
                trControl = ctrl)
table(predict(lda.fit),train$churn)
```

#RF
```{r}
rf.fit = train(as.factor(churn) ~ ., data=train, method="rf",
                trControl = trainControl(method = "cv"))
table(predict(rf.fit),train$churn)
```
```{r}
confusionMatrix(predict(rf.fit,test),factor(test$churn),positive = "1")

```

